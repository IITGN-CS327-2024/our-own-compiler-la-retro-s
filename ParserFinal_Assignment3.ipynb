{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U1xLm_HNRC4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class KulantLangLexer:\n",
        "    def __init__(self):\n",
        "        self.token_patterns = [\n",
        "            ('Keyword', r'\\b(int|word|bigint|char|dotie|bool|list|constant|tuple|if|otif|otw|for|while|get_out|go_on|return|void|try|catch|finally|display|input|start)\\b'),\n",
        "            ('Identifier', r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b'),\n",
        "            ('UnaryOperator', r'-|!|&|sizeof\\(\\)'),\n",
        "            ('BinaryOperator', r'==|!=|<=|>=|\\|\\||&&|[+\\-*/=](?!=)'),\n",
        "            ('LogicalLiteral', r'True|False'),\n",
        "            ('StringLiteral', r'\"([^\"]*)\"'),\n",
        "            ('Whitespace', r'\\s+'),\n",
        "            ('Paranthesis', r'[()\\{\\}\\[\\]]'),\n",
        "            ('Quotation', r'\"'),\n",
        "            ('Constant', r'[^\"{}\\s();]+'),\n",
        "            ('EndOfStatement', r';')\n",
        "        ]\n",
        "\n",
        "    def tokenize(self, source_code):\n",
        "        tokens = []\n",
        "        position = 0\n",
        "\n",
        "        while position < len(source_code):\n",
        "            match = None\n",
        "\n",
        "            for token_type, pattern in self.token_patterns:\n",
        "                regex = re.compile(pattern)\n",
        "                match = regex.match(source_code, position)\n",
        "\n",
        "                if match:\n",
        "                    if token_type == 'StringLiteral':\n",
        "                        tokens.append(('Quotation', '\"'))\n",
        "                        tokens.append(('StringLiteral', match.group(1)))\n",
        "                        tokens.append(('Quotation', '\"'))\n",
        "                    else:\n",
        "                        tokens.append((token_type, match.group(0)))\n",
        "                    position = match.end()\n",
        "                    break\n",
        "\n",
        "            if not match:\n",
        "                raise Exception(f\"Unexpected character at position {position}: {source_code[position]}\")\n",
        "\n",
        "        return tokens\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     filename = \"hello_world.lx\"\n",
        "#     with open(filename, 'r') as file:\n",
        "#         source_code = file.read()\n",
        "\n",
        "#     lexer = SimpleLangLexer()\n",
        "#     tokens = lexer.tokenize(source_code)\n",
        "\n",
        "#     print(tokens)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KulantLangParser:\n",
        "    def __init__(self):\n",
        "        self.tokens = []\n",
        "        self.current_token_index = 0\n",
        "\n",
        "    def parse(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.current_token_index = 0\n",
        "        self.print_tokens()  # Print the tokens before parsing\n",
        "        #ast = self.program()\n",
        "        #return ast\n",
        "\n",
        "    def print_tokens(self):\n",
        "        print(\"Tokens:\")\n",
        "        for token_type, token_value in self.tokens:\n",
        "            print(f\"{token_value}\")\n",
        "\n",
        "    # Rest of the parser implementation...\n",
        "\n",
        "def main():\n",
        "    filename = \"hello_world.lx\"\n",
        "    with open(filename, 'r') as file:\n",
        "        source_code = file.read()\n",
        "\n",
        "    lexer = KulantLangLexer()\n",
        "    tokens = lexer.tokenize(source_code)\n",
        "\n",
        "    parser = SimpleLangParser()\n",
        "    ast = parser.parse(tokens)\n",
        "\n",
        "    print(ast)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTew-4RlcnuP",
        "outputId": "c6c2b310-9077-4406-eff1-f5ec03b3c066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "void\n",
            " \n",
            "main\n",
            " \n",
            "(\n",
            ")\n",
            " \n",
            "{\n",
            " \n",
            "printf\n",
            "(\n",
            "\"\n",
            "hello, world\n",
            "\"\n",
            ")\n",
            ";\n",
            "}\n",
            "\n",
            "\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}